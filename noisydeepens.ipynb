{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy Deep Ensembleに関する実験用ソースコード\n",
    "\n",
    "・各種インポート\n",
    "\n",
    "・GPU接続\n",
    "\n",
    "・データセット/データローダー\n",
    "\n",
    "・モデル定義（ResNet18はCIFARに最適化、VGG16とEfficientNetB0はtorchvisionの実装）\n",
    "\n",
    "・学習/推論の関数定義\n",
    "\n",
    "・ノイズ付与の関数定義\n",
    "\n",
    "・実験\n",
    "\n",
    "の順になっています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
      "Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: torchinfo\n",
      "Successfully installed torchinfo-1.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各種インポート。使わないものもあるかもしれませんが全部載せておきます。\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pathlib\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import statistics\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import time\n",
    "import torchvision\n",
    "from collections import OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from torchvision import models\n",
    "from torchinfo import summary\n",
    "from functools import partial\n",
    "from typing import Any, Callable, List, Optional, Type, Union\n",
    "from torchvision.transforms._presets import ImageClassification\n",
    "from torchvision.utils import _log_api_usage_once\n",
    "from torchvision.models._api import register_model, Weights, WeightsEnum\n",
    "from torchvision.models._meta import _IMAGENET_CATEGORIES\n",
    "from torchvision.models._utils import _ovewrite_named_param, handle_legacy_interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# gpuに接続\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./CIFAR10_dataset/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:11<00:00, 14962655.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./CIFAR10_dataset/cifar-10-python.tar.gz to ./CIFAR10_dataset\n",
      "Files already downloaded and verified\n",
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./CIFAR100_dataset/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:10<00:00, 15441975.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./CIFAR100_dataset/cifar-100-python.tar.gz to ./CIFAR100_dataset\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み(ResNet18用)\n",
    "# CIFAR10にデータ拡張は適用していない。\n",
    "\n",
    "num_batch = 64\n",
    "\n",
    "# CIFAR10\n",
    "transform_cifar10_resnet18 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5],\n",
    "        [0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset_cifar10_resnet18 = datasets.CIFAR10(\n",
    "    './CIFAR10_dataset',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_cifar10_resnet18\n",
    ")\n",
    "\n",
    "test_dataset_cifar10_resnet18 = datasets.CIFAR10(\n",
    "    './CIFAR10_dataset',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_cifar10_resnet18\n",
    ")\n",
    "\n",
    "train_dataloader_cifar10_resnet18 = DataLoader(\n",
    "    train_dataset_cifar10_resnet18,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader_cifar10_resnet18 = DataLoader(\n",
    "    test_dataset_cifar10_resnet18,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "#CIFAR100\n",
    "transform_cifar100_resnet18 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5],\n",
    "        [0.5, 0.5, 0.5]\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=[-15,15])\n",
    "])\n",
    "\n",
    "transform_cifar100_resnet18_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5],\n",
    "        [0.5, 0.5, 0.5]\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset_cifar100_resnet18 = datasets.CIFAR100(\n",
    "    './CIFAR100_dataset',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_cifar100_resnet18\n",
    ")\n",
    "\n",
    "test_dataset_cifar100_resnet18 = datasets.CIFAR100(\n",
    "    './CIFAR100_dataset',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_cifar100_resnet18_test\n",
    ")\n",
    "\n",
    "train_dataloader_cifar100_resnet18 = DataLoader(\n",
    "    train_dataset_cifar100_resnet18,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader_cifar100_resnet18 = DataLoader(\n",
    "    test_dataset_cifar100_resnet18,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cifar10_resnet18 = [train_dataloader_cifar10_resnet18, test_dataloader_cifar10_resnet18]\n",
    "cifar100_resnet18 = [train_dataloader_cifar100_resnet18, test_dataloader_cifar100_resnet18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#データセットの読み込み(vgg16用)\n",
    "\n",
    "num_batch = 64\n",
    "\n",
    "#CIFAR10\n",
    "transform_cifar10_vgg16 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=[-15,15])\n",
    "])\n",
    "\n",
    "transform_cifar10_vgg16_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset_cifar10_vgg16 = datasets.CIFAR10(\n",
    "    './CIFAR10_dataset',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_cifar10_vgg16\n",
    ")\n",
    "\n",
    "test_dataset_cifar10_vgg16 = datasets.CIFAR10(\n",
    "    './CIFAR10_dataset',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_cifar10_vgg16_test\n",
    ")\n",
    "\n",
    "train_dataloader_cifar10_vgg16 = DataLoader(\n",
    "    train_dataset_cifar10_vgg16,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader_cifar10_vgg16 = DataLoader(\n",
    "    test_dataset_cifar10_vgg16,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "#CIFAR100\n",
    "transform_cifar100_vgg16 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5],\n",
    "        [0.5, 0.5, 0.5]\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=[-15,15])\n",
    "])\n",
    "\n",
    "transform_cifar100_vgg16_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5],\n",
    "        [0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset_cifar100_vgg16 = datasets.CIFAR100(\n",
    "    './CIFAR100_dataset',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_cifar100_vgg16\n",
    ")\n",
    "\n",
    "test_dataset_cifar100_vgg16 = datasets.CIFAR100(\n",
    "    './CIFAR100_dataset',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_cifar100_vgg16_test\n",
    ")\n",
    "\n",
    "train_dataloader_cifar100_vgg16 = DataLoader(\n",
    "    train_dataset_cifar100_vgg16,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader_cifar100_vgg16 = DataLoader(\n",
    "    test_dataset_cifar100_vgg16,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cifar10_vgg16 = [train_dataloader_cifar10_vgg16, test_dataloader_cifar10_vgg16]\n",
    "cifar100_vgg16 = [train_dataloader_cifar100_vgg16, test_dataloader_cifar100_vgg16]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み(EfficientNetB0用)\n",
    "# B0に入力する画像のサイズは224x224が適しているらしいので、ここで32x32→224x224に変換している。\n",
    "\n",
    "num_batch = 512\n",
    "\n",
    "# CIFAR10\n",
    "\n",
    "transform_cifar10_efficient = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224, 224), antialias=True),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=[-15,15])\n",
    "])\n",
    "\n",
    "transform_cifar10_efficient_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224, 224), antialias=True),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset_cifar10_efficient = datasets.CIFAR10(\n",
    "    './CIFAR10_dataset',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_cifar10_efficient\n",
    ")\n",
    "\n",
    "test_dataset_cifar10_efficient = datasets.CIFAR10(\n",
    "    './CIFAR10_dataset',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_cifar10_efficient_test\n",
    ")\n",
    "\n",
    "train_dataloader_cifar10_efficient = DataLoader(\n",
    "    train_dataset_cifar10_efficient,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader_cifar10_efficient = DataLoader(\n",
    "    test_dataset_cifar10_efficient,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "# CIFAR100\n",
    "\n",
    "transform_cifar100_efficient = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224, 224), antialias=True),\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5],\n",
    "        [0.5, 0.5, 0.5]\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(degrees=[-15,15])\n",
    "])\n",
    "\n",
    "transform_cifar100_efficient_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(224, 224), antialias=True),\n",
    "    transforms.Normalize(\n",
    "        [0.5, 0.5, 0.5],\n",
    "        [0.5, 0.5, 0.5]\n",
    "    )\n",
    "])\n",
    "\n",
    "train_dataset_cifar100_efficient = datasets.CIFAR100(\n",
    "    './CIFAR100_dataset',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform_cifar100_efficient\n",
    ")\n",
    "\n",
    "test_dataset_cifar100_efficient = datasets.CIFAR100(\n",
    "    './CIFAR100_dataset',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform_cifar100_efficient_test\n",
    ")\n",
    "\n",
    "train_dataloader_cifar100_efficient = DataLoader(\n",
    "    train_dataset_cifar100_efficient,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader_cifar100_efficient = DataLoader(\n",
    "    test_dataset_cifar100_efficient,\n",
    "    batch_size=num_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "cifar10_efficient = [train_dataloader_cifar10_efficient, test_dataloader_cifar10_efficient]\n",
    "cifar100_efficient = [train_dataloader_cifar100_efficient, test_dataloader_cifar100_efficient]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet18\n",
    "def conv3x3(in_planes: int, out_planes: int, stride: int = 1, groups: int = 1, dilation: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(\n",
    "        in_planes,\n",
    "        out_planes,\n",
    "        kernel_size=3,\n",
    "        stride=stride,\n",
    "        padding=dilation,\n",
    "        groups=groups,\n",
    "        bias=False,\n",
    "        dilation=dilation,\n",
    "    )\n",
    "\n",
    "def conv1x1(in_planes: int, out_planes: int, stride: int = 1) -> nn.Conv2d:\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion: int = 1\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError(\"BasicBlock only supports groups=1 and base_width=64\")\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    # Bottleneck in torchvision places the stride for downsampling at 3x3 convolution(self.conv2)\n",
    "    # while original implementation places the stride at the first 1x1 convolution(self.conv1)\n",
    "    # according to \"Deep residual learning for image recognition\" https://arxiv.org/abs/1512.03385.\n",
    "    # This variant is also known as ResNet V1.5 and improves accuracy according to\n",
    "    # https://ngc.nvidia.com/catalog/model-scripts/nvidia:resnet_50_v1_5_for_pytorch.\n",
    "\n",
    "    expansion: int = 4\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inplanes: int,\n",
    "        planes: int,\n",
    "        stride: int = 1,\n",
    "        downsample: Optional[nn.Module] = None,\n",
    "        groups: int = 1,\n",
    "        base_width: int = 64,\n",
    "        dilation: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 64.0)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        layers: List[int],\n",
    "        num_classes: int = 1000,\n",
    "        zero_init_residual: bool = False,\n",
    "        groups: int = 1,\n",
    "        width_per_group: int = 64,\n",
    "        replace_stride_with_dilation: Optional[List[bool]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        _log_api_usage_once(self)\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\n",
    "                \"replace_stride_with_dilation should be None \"\n",
    "                f\"or a 3-element tuple, got {replace_stride_with_dilation}\"\n",
    "            )\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck) and m.bn3.weight is not None:\n",
    "                    nn.init.constant_(m.bn3.weight, 0)  # type: ignore[arg-type]\n",
    "                elif isinstance(m, BasicBlock) and m.bn2.weight is not None:\n",
    "                    nn.init.constant_(m.bn2.weight, 0)  # type: ignore[arg-type]\n",
    "\n",
    "    def _make_layer(\n",
    "        self,\n",
    "        block: Type[Union[BasicBlock, Bottleneck]],\n",
    "        planes: int,\n",
    "        blocks: int,\n",
    "        stride: int = 1,\n",
    "        dilate: bool = False,\n",
    "    ) -> nn.Sequential:\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(\n",
    "            block(\n",
    "                self.inplanes, planes, stride, downsample, self.groups, self.base_width, previous_dilation, norm_layer\n",
    "            )\n",
    "        )\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(\n",
    "                block(\n",
    "                    self.inplanes,\n",
    "                    planes,\n",
    "                    groups=self.groups,\n",
    "                    base_width=self.base_width,\n",
    "                    dilation=self.dilation,\n",
    "                    norm_layer=norm_layer,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        #--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "        # x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(\n",
    "    block: Type[Union[BasicBlock, Bottleneck]],\n",
    "    layers: List[int],\n",
    "    weights: Optional[WeightsEnum],\n",
    "    progress: bool,\n",
    "    **kwargs: Any,\n",
    ") -> ResNet:\n",
    "    if weights is not None:\n",
    "        _ovewrite_named_param(kwargs, \"num_classes\", len(weights.meta[\"categories\"]))\n",
    "\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "\n",
    "    if weights is not None:\n",
    "        model.load_state_dict(weights.get_state_dict(progress=progress))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "_COMMON_META = {\n",
    "    \"min_size\": (1, 1),\n",
    "    \"categories\": _IMAGENET_CATEGORIES,\n",
    "}\n",
    "\n",
    "def resnet18(num_classes):\n",
    "    model = ResNet(BasicBlock, [2,2,2,2], num_classes=num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG16\n",
    "def vgg16(num_classes):\n",
    "    model = models.vgg16(num_classes=num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EfficientNetB0\n",
    "def efficientnetb0(num_classes):\n",
    "    model = models.efficientnet_b0(num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(model, dataset, epochs, lr, log=True, acc=False):\n",
    "    log_train_acc = []\n",
    "    log_train_loss = []\n",
    "    log_test_acc = []\n",
    "    log_test_loss = []\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # SGD　損失関数　学習率スケジューラの設定\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_acc = 0\n",
    "        train_loss = 0\n",
    "        test_acc = 0\n",
    "        test_loss = 0\n",
    "\n",
    "        model.train()\n",
    "        for inputs , labels in dataset[0]:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "            train_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataset[1]:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                test_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "                \n",
    "        log_train_loss.append(train_loss / len(dataset[0]))\n",
    "        log_test_loss.append(test_loss / len(dataset[1]))\n",
    "        # Accuracyを計算\n",
    "        log_train_acc.append(train_acc / len(dataset[0].dataset))\n",
    "        log_test_acc.append(test_acc / len(dataset[1].dataset))\n",
    "\n",
    "        if log == True:\n",
    "            print(\"Epoch[{}/{}], train_acc:{acc:.4f}, train_loss:{loss:.4f}, test_acc:{test_acc:.4f}, test_loss:{test_loss:.4f}\"\n",
    "                        .format(epoch+1, epochs, acc=log_train_acc[-1], loss=log_train_loss[-1], test_acc=log_test_acc[-1], test_loss=log_test_loss[-1]))\n",
    "    \n",
    "    if acc == True:\n",
    "        return log_test_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(model, dataset, acc=False):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    log_test_acc = []\n",
    "    log_test_loss = []\n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataset[1]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "        \n",
    "    log_test_loss.append(test_loss / len(dataset[1]))\n",
    "    log_test_acc.append(test_acc / len(dataset[1].dataset))\n",
    "\n",
    "    print(\"test_acc:{val_acc:.4f}, test_loss:{val_loss:.4f}\"\n",
    "                  .format(val_acc=log_test_acc[-1], val_loss=log_test_loss[-1]))\n",
    "    \n",
    "    if acc == True:\n",
    "        return log_test_acc[-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ens_test(model_list, dataset, acc=False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    log_test_acc = []\n",
    "    log_test_loss = []\n",
    "    test_acc = 0\n",
    "    test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataset[1]:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model_list[0](inputs)\n",
    "            for i in range(len(model_list)-1):\n",
    "                outputs += model_list[i+1](inputs)\n",
    "            outputs = outputs / len(model_list)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            test_acc += (outputs.max(1)[1] == labels).sum().item()\n",
    "    \n",
    "    log_test_loss.append(test_loss / len(dataset[1]))\n",
    "    log_test_acc.append(test_acc / len(dataset[1].dataset))\n",
    "    print(\"test_acc:{val_acc:.4f}, test_loss:{val_loss:.4f}\"\n",
    "                  .format(val_acc=log_test_acc[-1], val_loss=log_test_loss[-1]))\n",
    "    \n",
    "    if acc == True:\n",
    "        return log_test_acc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 各モデルアーキテクチャのノイズ付与対象レイヤーのリスト。\n",
    "\n",
    "resnet18_layer_list = ['conv1.weight', \n",
    "              'layer1.0.conv1.weight',\n",
    "              'layer1.0.conv2.weight',\n",
    "              'layer1.1.conv1.weight',\n",
    "              'layer1.1.conv2.weight',\n",
    "\n",
    "              'layer2.0.conv1.weight',\n",
    "              'layer2.0.conv2.weight',\n",
    "              'layer2.1.conv1.weight',\n",
    "              'layer2.1.conv2.weight',\n",
    "\n",
    "              'layer3.0.conv1.weight',\n",
    "              'layer3.0.conv2.weight',\n",
    "              'layer3.1.conv1.weight',\n",
    "              'layer3.1.conv2.weight',\n",
    "              \n",
    "              'layer4.0.conv1.weight',\n",
    "              'layer4.0.conv2.weight',\n",
    "              'layer4.1.conv1.weight',\n",
    "              'layer4.1.conv2.weight',\n",
    "              'fc.weight'\n",
    "              ]\n",
    "\n",
    "vgg16_layer_list = ['features.0.weight',\n",
    "                'features.2.weight',\n",
    "                'features.5.weight',\n",
    "                'features.7.weight',\n",
    "                'features.10.weight',\n",
    "                'features.12.weight',\n",
    "                'features.14.weight',\n",
    "                'features.17.weight',\n",
    "                'features.19.weight',\n",
    "                'features.21.weight',\n",
    "                'features.24.weight',\n",
    "                'features.26.weight',\n",
    "                'features.28.weight',\n",
    "                'classifier.0.weight',\n",
    "                'classifier.3.weight',\n",
    "                'classifier.6.weight'\n",
    "                ]\n",
    "\n",
    "efficientnetb0_layer_list = ['features.0.0.weight',\n",
    "                'features.0.1.weight',\n",
    "                'features.1.0.block.0.0.weight',\n",
    "                'features.1.0.block.0.1.weight',\n",
    "                'features.1.0.block.1.fc1.weight',\n",
    "                'features.1.0.block.1.fc2.weight',\n",
    "                'features.1.0.block.2.0.weight',\n",
    "                'features.1.0.block.2.1.weight',\n",
    "                'features.2.0.block.0.0.weight',\n",
    "                'features.2.0.block.0.1.weight',\n",
    "                'features.2.0.block.1.0.weight',\n",
    "                'features.2.0.block.1.1.weight',\n",
    "                'features.2.0.block.2.fc1.weight',\n",
    "                'features.2.0.block.2.fc2.weight',\n",
    "                'features.2.0.block.3.0.weight',\n",
    "                'features.2.0.block.3.1.weight',\n",
    "                'features.2.1.block.0.0.weight',\n",
    "                'features.2.1.block.0.1.weight',\n",
    "                'features.2.1.block.1.0.weight',\n",
    "                'features.2.1.block.1.1.weight',\n",
    "                'features.2.1.block.2.fc1.weight',\n",
    "                'features.2.1.block.2.fc2.weight',\n",
    "                'features.2.1.block.3.0.weight',\n",
    "                'features.2.1.block.3.1.weight',\n",
    "                'features.3.0.block.0.0.weight',\n",
    "                'features.3.0.block.0.1.weight',\n",
    "                'features.3.0.block.1.0.weight',\n",
    "                'features.3.0.block.1.1.weight',\n",
    "                'features.3.0.block.2.fc1.weight',\n",
    "                'features.3.0.block.2.fc2.weight',\n",
    "                'features.3.0.block.3.0.weight',\n",
    "                'features.3.0.block.3.1.weight',\n",
    "                'features.3.1.block.0.0.weight',\n",
    "                'features.3.1.block.0.1.weight',\n",
    "                'features.3.1.block.1.0.weight',\n",
    "                'features.3.1.block.1.1.weight',\n",
    "                'features.3.1.block.2.fc1.weight',\n",
    "                'features.3.1.block.2.fc2.weight',\n",
    "                'features.3.1.block.3.0.weight',\n",
    "                'features.3.1.block.3.1.weight',\n",
    "                'features.4.0.block.0.0.weight',\n",
    "                'features.4.0.block.0.1.weight',\n",
    "                'features.4.0.block.1.0.weight',\n",
    "                'features.4.0.block.1.1.weight',\n",
    "                'features.4.0.block.2.fc1.weight',\n",
    "                'features.4.0.block.2.fc2.weight',\n",
    "                'features.4.0.block.3.0.weight',\n",
    "                'features.4.0.block.3.1.weight',\n",
    "                'features.4.1.block.0.0.weight',\n",
    "                'features.4.1.block.0.1.weight',\n",
    "                'features.4.1.block.1.0.weight',\n",
    "                'features.4.1.block.1.1.weight',\n",
    "                'features.4.1.block.2.fc1.weight',\n",
    "                'features.4.1.block.2.fc2.weight',\n",
    "                'features.4.1.block.3.0.weight',\n",
    "                'features.4.1.block.3.1.weight',\n",
    "                'features.4.2.block.0.0.weight',\n",
    "                'features.4.2.block.0.1.weight',\n",
    "                'features.4.2.block.1.0.weight',\n",
    "                'features.4.2.block.1.1.weight',\n",
    "                'features.4.2.block.2.fc1.weight',\n",
    "                'features.4.2.block.2.fc2.weight',\n",
    "                'features.4.2.block.3.0.weight',\n",
    "                'features.4.2.block.3.1.weight',\n",
    "                'features.5.0.block.0.0.weight',\n",
    "                'features.5.0.block.0.1.weight',\n",
    "                'features.5.0.block.1.0.weight',\n",
    "                'features.5.0.block.1.1.weight',\n",
    "                'features.5.0.block.2.fc1.weight',\n",
    "                'features.5.0.block.2.fc2.weight',\n",
    "                'features.5.0.block.3.0.weight',\n",
    "                'features.5.0.block.3.1.weight',\n",
    "                'features.5.1.block.0.0.weight',\n",
    "                'features.5.1.block.0.1.weight',\n",
    "                'features.5.1.block.1.0.weight',\n",
    "                'features.5.1.block.1.1.weight',\n",
    "                'features.5.1.block.2.fc1.weight',\n",
    "                'features.5.1.block.2.fc2.weight',\n",
    "                'features.5.1.block.3.0.weight',\n",
    "                'features.5.1.block.3.1.weight',\n",
    "                'features.5.2.block.0.0.weight',\n",
    "                'features.5.2.block.0.1.weight',\n",
    "                'features.5.2.block.1.0.weight',\n",
    "                'features.5.2.block.1.1.weight',\n",
    "                'features.5.2.block.2.fc1.weight',\n",
    "                'features.5.2.block.2.fc2.weight',\n",
    "                'features.5.2.block.3.0.weight',\n",
    "                'features.5.2.block.3.1.weight',\n",
    "                'features.6.0.block.0.0.weight',\n",
    "                'features.6.0.block.0.1.weight',\n",
    "                'features.6.0.block.1.0.weight',\n",
    "                'features.6.0.block.1.1.weight',\n",
    "                'features.6.0.block.2.fc1.weight',\n",
    "                'features.6.0.block.2.fc2.weight',\n",
    "                'features.6.0.block.3.0.weight',\n",
    "                'features.6.0.block.3.1.weight',\n",
    "                'features.6.1.block.0.0.weight',\n",
    "                'features.6.1.block.0.1.weight',\n",
    "                'features.6.1.block.1.0.weight',\n",
    "                'features.6.1.block.1.1.weight',\n",
    "                'features.6.1.block.2.fc1.weight',\n",
    "                'features.6.1.block.2.fc2.weight',\n",
    "                'features.6.1.block.3.0.weight',\n",
    "                'features.6.1.block.3.1.weight',\n",
    "                'features.6.2.block.0.0.weight',\n",
    "                'features.6.2.block.0.1.weight',\n",
    "                'features.6.2.block.1.0.weight',\n",
    "                'features.6.2.block.1.1.weight',\n",
    "                'features.6.2.block.2.fc1.weight',\n",
    "                'features.6.2.block.2.fc2.weight',\n",
    "                'features.6.2.block.3.0.weight',\n",
    "                'features.6.2.block.3.1.weight',\n",
    "                'features.6.3.block.0.0.weight',\n",
    "                'features.6.3.block.0.1.weight',\n",
    "                'features.6.3.block.1.0.weight',\n",
    "                'features.6.3.block.1.1.weight',\n",
    "                'features.6.3.block.2.fc1.weight',\n",
    "                'features.6.3.block.2.fc2.weight',\n",
    "                'features.6.3.block.3.0.weight',\n",
    "                'features.6.3.block.3.1.weight',\n",
    "                'features.7.0.block.0.0.weight',\n",
    "                'features.7.0.block.0.1.weight',\n",
    "                'features.7.0.block.1.0.weight',\n",
    "                'features.7.0.block.1.1.weight',\n",
    "                'features.7.0.block.2.fc1.weight',\n",
    "                'features.7.0.block.2.fc2.weight',\n",
    "                'features.7.0.block.3.0.weight',\n",
    "                'features.7.0.block.3.1.weight',\n",
    "                'features.8.0.weight',\n",
    "                'features.8.1.weight',\n",
    "                'classifier.1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一様分布ノイズ(uni noise)\n",
    "\n",
    "def make_uni_noise(weight, a, b):\n",
    "    w = weight.cpu().detach().numpy().copy()\n",
    "    size = w.size\n",
    "    o_shape = w.shape\n",
    "    w = w.reshape(-1)\n",
    "    p = np.random.choice(size, int(size*a), replace=False)\n",
    "    \n",
    "    for i in range(p.size):\n",
    "        noise = random.uniform(-b, b)\n",
    "        tmp = w[p[i]]\n",
    "        w[p[i]] += noise\n",
    "        if(tmp * w[p[i]] < 0):\n",
    "            w[p[i]] *= -1\n",
    "\n",
    "    w = w.reshape(o_shape)\n",
    "    new_weight = torch.from_numpy(w).clone()\n",
    "\n",
    "    return new_weight\n",
    "\n",
    "def uni_noise(model, model_arc, a, b):\n",
    "    new_model = copy.deepcopy(model)\n",
    "    if model_arc == 'resnet18':\n",
    "        l_list = resnet18_layer_list\n",
    "    elif model_arc == 'vgg16':\n",
    "        l_list = vgg16_layer_list\n",
    "    elif model_arc == 'efficientnetb0':\n",
    "        l_list = efficientnetb0_layer_list\n",
    "    else: print('no such a model')\n",
    "\n",
    "    for l in l_list:\n",
    "        new_model.state_dict()[l][0:new_model.state_dict()[l].shape[0]] = make_uni_noise(new_model.state_dict()[l], a, b)\n",
    "    \n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規分布ノイズ(norm noise)\n",
    "\n",
    "def make_norm_noise(weight, a, mean, b):\n",
    "    w = weight.cpu().detach().numpy().copy()\n",
    "    size = w.size\n",
    "    o_shape = w.shape\n",
    "    w = w.reshape(-1)\n",
    "    p = np.random.choice(size, int(size*a), replace=False)\n",
    "    \n",
    "    for i in range(p.size):\n",
    "        noise = random.normalvariate(mean, b)\n",
    "        tmp = w[p[i]]\n",
    "        w[p[i]] += noise\n",
    "        if(tmp * w[p[i]] < 0):\n",
    "            w[p[i]] *= -1\n",
    "    \n",
    "    w = w.reshape(o_shape)\n",
    "    new_weight = torch.from_numpy(w).clone()\n",
    "\n",
    "    return new_weight\n",
    "\n",
    "def norm_noise(model, model_arc, a, mean, b):\n",
    "    new_model = copy.deepcopy(model)\n",
    "    if model_arc == 'resnet18':\n",
    "        l_list = resnet18_layer_list\n",
    "    elif model_arc == 'vgg16':\n",
    "        l_list = vgg16_layer_list\n",
    "    elif model_arc == 'efficientnetb0':\n",
    "        l_list = efficientnetb0_layer_list\n",
    "    else: print('no such a model')\n",
    "\n",
    "    for l in l_list:\n",
    "        new_model.state_dict()[l][0:new_model.state_dict()[l].shape[0]] = make_norm_noise(new_model.state_dict()[l], a, mean, b)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実験\n",
    "\n",
    "#1行目は「ノイズ手法」、「データセット」、「モデルアーキテクチャ」の順に表示。\n",
    "\n",
    "XXX_acc_list_XXX：M = 1 ~ 10の場合のaccuracyを格納したリスト。\n",
    "\n",
    "XXX_model_list_XXX：アンサンブルを構成するモデルを格納したリスト。\n",
    "\n",
    "アンサンブルメンバ数Mは10。\n",
    "\n",
    "関数uni_noise：一様分布に基づくノイズを付与。引数は左から「ノイズ付与するモデル」、「モデルアーキテクチャ」、「ノイズ付与率α」、「ノイズ強度β」。返り値はモデル。\n",
    "\n",
    "関数norm_noise：正規分布に基づくノイズを付与。引数は左から「ノイズ付与するモデル」、「モデルアーキテクチャ」、「ノイズ付与率α」、「分布の平均（0固定でよい）」、「ノイズ強度β」。返り値はモデル。\n",
    "\n",
    "関数model_train：モデルの学習。引数は左から「学習するモデル」、「データセット」、「エポック」、「学習率」、「学習のログ出力の有無」。\n",
    "\n",
    "関数model_test：単一モデルのテスト。引数は左から「モデル」、「データセット」、「返り値としてaccの値を返すか（True固定でよい）」。\n",
    "\n",
    "関数ens_test：アンサンブルモデルのテスト。引数は左から「モデルのリスト」、「データセット」、「返り値としてaccの値を返すか（True固定でよい）」。\n",
    "\n",
    "SGD、損失関数、学習率スケジューラの設定に関しては関数model_trainの定義で\n",
    "ノイズを伴う手法に関しては付属のファイルから事前学習モデルをロードする必要あり\n",
    "ハイパーパラメータは本実験で使用したままになっている"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[1/200], train_acc:0.3253, train_loss:1.9573, test_acc:0.4116, test_loss:1.6175\n",
      "Epoch[2/200], train_acc:0.5186, train_loss:1.3248, test_acc:0.5745, test_loss:1.1722\n",
      "Epoch[3/200], train_acc:0.6491, train_loss:0.9898, test_acc:0.6623, test_loss:0.9900\n",
      "Epoch[4/200], train_acc:0.7347, train_loss:0.7589, test_acc:0.7425, test_loss:0.7620\n",
      "Epoch[5/200], train_acc:0.7754, train_loss:0.6502, test_acc:0.7124, test_loss:0.8317\n",
      "Epoch[6/200], train_acc:0.7959, train_loss:0.5850, test_acc:0.7726, test_loss:0.6704\n",
      "Epoch[7/200], train_acc:0.8144, train_loss:0.5365, test_acc:0.7637, test_loss:0.6969\n",
      "Epoch[8/200], train_acc:0.8250, train_loss:0.5051, test_acc:0.7461, test_loss:0.7595\n",
      "Epoch[9/200], train_acc:0.8358, train_loss:0.4783, test_acc:0.7405, test_loss:0.8240\n",
      "Epoch[10/200], train_acc:0.8442, train_loss:0.4515, test_acc:0.7763, test_loss:0.6765\n",
      "Epoch[11/200], train_acc:0.8444, train_loss:0.4481, test_acc:0.7534, test_loss:0.7607\n",
      "Epoch[12/200], train_acc:0.8518, train_loss:0.4289, test_acc:0.7934, test_loss:0.6139\n",
      "Epoch[13/200], train_acc:0.8553, train_loss:0.4161, test_acc:0.7905, test_loss:0.6145\n",
      "Epoch[14/200], train_acc:0.8595, train_loss:0.4051, test_acc:0.8012, test_loss:0.5952\n",
      "Epoch[15/200], train_acc:0.8641, train_loss:0.3963, test_acc:0.7897, test_loss:0.6287\n",
      "Epoch[16/200], train_acc:0.8633, train_loss:0.3979, test_acc:0.7731, test_loss:0.6897\n",
      "Epoch[17/200], train_acc:0.8687, train_loss:0.3835, test_acc:0.7939, test_loss:0.6268\n",
      "Epoch[18/200], train_acc:0.8699, train_loss:0.3784, test_acc:0.7786, test_loss:0.7331\n",
      "Epoch[19/200], train_acc:0.8699, train_loss:0.3781, test_acc:0.8027, test_loss:0.5833\n",
      "Epoch[20/200], train_acc:0.8708, train_loss:0.3740, test_acc:0.7716, test_loss:0.7178\n",
      "Epoch[21/200], train_acc:0.8749, train_loss:0.3639, test_acc:0.7825, test_loss:0.6523\n",
      "Epoch[22/200], train_acc:0.8746, train_loss:0.3654, test_acc:0.7126, test_loss:1.0090\n",
      "Epoch[23/200], train_acc:0.8756, train_loss:0.3612, test_acc:0.8189, test_loss:0.5404\n",
      "Epoch[24/200], train_acc:0.8775, train_loss:0.3575, test_acc:0.7741, test_loss:0.6553\n",
      "Epoch[25/200], train_acc:0.8785, train_loss:0.3526, test_acc:0.7748, test_loss:0.6937\n",
      "Epoch[26/200], train_acc:0.8795, train_loss:0.3524, test_acc:0.7722, test_loss:0.6950\n",
      "Epoch[27/200], train_acc:0.8811, train_loss:0.3486, test_acc:0.7856, test_loss:0.6686\n",
      "Epoch[28/200], train_acc:0.8801, train_loss:0.3469, test_acc:0.7923, test_loss:0.6443\n",
      "Epoch[29/200], train_acc:0.8842, train_loss:0.3391, test_acc:0.8065, test_loss:0.5886\n",
      "Epoch[30/200], train_acc:0.8828, train_loss:0.3402, test_acc:0.7555, test_loss:0.7821\n",
      "Epoch[31/200], train_acc:0.8841, train_loss:0.3375, test_acc:0.7637, test_loss:0.7120\n",
      "Epoch[32/200], train_acc:0.8851, train_loss:0.3355, test_acc:0.7977, test_loss:0.6051\n",
      "Epoch[33/200], train_acc:0.8896, train_loss:0.3243, test_acc:0.8131, test_loss:0.5734\n",
      "Epoch[34/200], train_acc:0.8847, train_loss:0.3357, test_acc:0.7403, test_loss:0.8324\n",
      "Epoch[35/200], train_acc:0.8868, train_loss:0.3266, test_acc:0.7709, test_loss:0.7502\n",
      "Epoch[36/200], train_acc:0.8865, train_loss:0.3298, test_acc:0.8018, test_loss:0.6191\n",
      "Epoch[37/200], train_acc:0.8900, train_loss:0.3205, test_acc:0.7747, test_loss:0.7035\n",
      "Epoch[38/200], train_acc:0.8904, train_loss:0.3182, test_acc:0.7585, test_loss:0.7551\n",
      "Epoch[39/200], train_acc:0.8918, train_loss:0.3153, test_acc:0.7835, test_loss:0.6811\n",
      "Epoch[40/200], train_acc:0.8921, train_loss:0.3146, test_acc:0.7841, test_loss:0.6705\n",
      "Epoch[41/200], train_acc:0.8927, train_loss:0.3139, test_acc:0.7858, test_loss:0.6944\n",
      "Epoch[42/200], train_acc:0.8918, train_loss:0.3135, test_acc:0.8198, test_loss:0.5505\n",
      "Epoch[43/200], train_acc:0.8946, train_loss:0.3052, test_acc:0.7772, test_loss:0.6982\n",
      "Epoch[44/200], train_acc:0.8970, train_loss:0.3016, test_acc:0.7649, test_loss:0.7606\n",
      "Epoch[45/200], train_acc:0.8961, train_loss:0.3036, test_acc:0.7830, test_loss:0.6799\n",
      "Epoch[46/200], train_acc:0.8990, train_loss:0.2980, test_acc:0.7887, test_loss:0.6944\n",
      "Epoch[47/200], train_acc:0.8949, train_loss:0.3084, test_acc:0.8210, test_loss:0.5601\n",
      "Epoch[48/200], train_acc:0.9001, train_loss:0.2923, test_acc:0.7652, test_loss:0.7053\n",
      "Epoch[49/200], train_acc:0.9001, train_loss:0.2944, test_acc:0.7632, test_loss:0.8133\n",
      "Epoch[50/200], train_acc:0.8996, train_loss:0.2954, test_acc:0.7724, test_loss:0.7033\n",
      "Epoch[51/200], train_acc:0.8992, train_loss:0.2955, test_acc:0.7843, test_loss:0.6858\n",
      "Epoch[52/200], train_acc:0.9026, train_loss:0.2834, test_acc:0.7905, test_loss:0.6531\n",
      "Epoch[53/200], train_acc:0.9021, train_loss:0.2866, test_acc:0.8108, test_loss:0.5891\n",
      "Epoch[54/200], train_acc:0.9019, train_loss:0.2857, test_acc:0.7892, test_loss:0.6664\n",
      "Epoch[55/200], train_acc:0.9046, train_loss:0.2800, test_acc:0.8172, test_loss:0.5887\n",
      "Epoch[56/200], train_acc:0.9058, train_loss:0.2759, test_acc:0.7798, test_loss:0.7120\n",
      "Epoch[57/200], train_acc:0.9067, train_loss:0.2729, test_acc:0.7864, test_loss:0.6514\n",
      "Epoch[58/200], train_acc:0.9062, train_loss:0.2724, test_acc:0.7870, test_loss:0.7017\n",
      "Epoch[59/200], train_acc:0.9055, train_loss:0.2744, test_acc:0.8020, test_loss:0.6149\n",
      "Epoch[60/200], train_acc:0.9097, train_loss:0.2624, test_acc:0.7966, test_loss:0.6402\n",
      "Epoch[61/200], train_acc:0.9081, train_loss:0.2651, test_acc:0.8181, test_loss:0.5764\n",
      "Epoch[62/200], train_acc:0.9083, train_loss:0.2654, test_acc:0.8103, test_loss:0.6037\n",
      "Epoch[63/200], train_acc:0.9115, train_loss:0.2584, test_acc:0.8028, test_loss:0.6363\n",
      "Epoch[64/200], train_acc:0.9131, train_loss:0.2569, test_acc:0.7995, test_loss:0.6126\n",
      "Epoch[65/200], train_acc:0.9132, train_loss:0.2567, test_acc:0.7912, test_loss:0.6729\n",
      "Epoch[66/200], train_acc:0.9124, train_loss:0.2557, test_acc:0.7667, test_loss:0.7984\n",
      "Epoch[67/200], train_acc:0.9138, train_loss:0.2522, test_acc:0.7926, test_loss:0.6645\n",
      "Epoch[68/200], train_acc:0.9149, train_loss:0.2472, test_acc:0.8154, test_loss:0.5794\n",
      "Epoch[69/200], train_acc:0.9165, train_loss:0.2453, test_acc:0.7966, test_loss:0.6431\n",
      "Epoch[70/200], train_acc:0.9180, train_loss:0.2387, test_acc:0.7886, test_loss:0.7040\n",
      "Epoch[71/200], train_acc:0.9185, train_loss:0.2382, test_acc:0.8093, test_loss:0.6037\n",
      "Epoch[72/200], train_acc:0.9159, train_loss:0.2436, test_acc:0.7998, test_loss:0.6319\n",
      "Epoch[73/200], train_acc:0.9212, train_loss:0.2303, test_acc:0.7563, test_loss:0.7968\n",
      "Epoch[74/200], train_acc:0.9214, train_loss:0.2331, test_acc:0.8146, test_loss:0.5825\n",
      "Epoch[75/200], train_acc:0.9231, train_loss:0.2216, test_acc:0.8057, test_loss:0.6232\n",
      "Epoch[76/200], train_acc:0.9232, train_loss:0.2228, test_acc:0.8041, test_loss:0.6339\n",
      "Epoch[77/200], train_acc:0.9224, train_loss:0.2223, test_acc:0.8046, test_loss:0.6165\n",
      "Epoch[78/200], train_acc:0.9246, train_loss:0.2206, test_acc:0.8039, test_loss:0.6328\n",
      "Epoch[79/200], train_acc:0.9269, train_loss:0.2124, test_acc:0.8150, test_loss:0.6129\n",
      "Epoch[80/200], train_acc:0.9274, train_loss:0.2118, test_acc:0.8266, test_loss:0.5687\n",
      "Epoch[81/200], train_acc:0.9275, train_loss:0.2129, test_acc:0.7844, test_loss:0.7302\n",
      "Epoch[82/200], train_acc:0.9279, train_loss:0.2088, test_acc:0.8000, test_loss:0.6505\n",
      "Epoch[83/200], train_acc:0.9293, train_loss:0.2072, test_acc:0.7937, test_loss:0.7086\n",
      "Epoch[84/200], train_acc:0.9297, train_loss:0.2054, test_acc:0.8165, test_loss:0.6001\n",
      "Epoch[85/200], train_acc:0.9317, train_loss:0.2014, test_acc:0.8041, test_loss:0.6433\n",
      "Epoch[86/200], train_acc:0.9339, train_loss:0.1925, test_acc:0.8171, test_loss:0.6134\n",
      "Epoch[87/200], train_acc:0.9345, train_loss:0.1913, test_acc:0.7999, test_loss:0.6650\n",
      "Epoch[88/200], train_acc:0.9361, train_loss:0.1887, test_acc:0.8200, test_loss:0.6022\n",
      "Epoch[89/200], train_acc:0.9351, train_loss:0.1923, test_acc:0.8127, test_loss:0.5921\n",
      "Epoch[90/200], train_acc:0.9386, train_loss:0.1834, test_acc:0.8139, test_loss:0.5970\n",
      "Epoch[91/200], train_acc:0.9377, train_loss:0.1809, test_acc:0.8006, test_loss:0.6665\n",
      "Epoch[92/200], train_acc:0.9395, train_loss:0.1776, test_acc:0.7885, test_loss:0.6893\n",
      "Epoch[93/200], train_acc:0.9403, train_loss:0.1757, test_acc:0.7877, test_loss:0.7713\n",
      "Epoch[94/200], train_acc:0.9376, train_loss:0.1848, test_acc:0.7945, test_loss:0.7440\n",
      "Epoch[95/200], train_acc:0.9453, train_loss:0.1641, test_acc:0.8096, test_loss:0.6405\n",
      "Epoch[96/200], train_acc:0.9421, train_loss:0.1709, test_acc:0.8172, test_loss:0.6011\n",
      "Epoch[97/200], train_acc:0.9440, train_loss:0.1664, test_acc:0.8192, test_loss:0.6009\n",
      "Epoch[98/200], train_acc:0.9449, train_loss:0.1645, test_acc:0.8113, test_loss:0.6144\n",
      "Epoch[99/200], train_acc:0.9492, train_loss:0.1522, test_acc:0.8112, test_loss:0.6587\n",
      "Epoch[100/200], train_acc:0.9456, train_loss:0.1586, test_acc:0.8318, test_loss:0.5496\n",
      "Epoch[101/200], train_acc:0.9507, train_loss:0.1487, test_acc:0.8140, test_loss:0.6475\n",
      "Epoch[102/200], train_acc:0.9497, train_loss:0.1503, test_acc:0.8251, test_loss:0.5782\n",
      "Epoch[103/200], train_acc:0.9501, train_loss:0.1475, test_acc:0.8237, test_loss:0.5866\n",
      "Epoch[104/200], train_acc:0.9502, train_loss:0.1462, test_acc:0.8129, test_loss:0.6479\n",
      "Epoch[105/200], train_acc:0.9504, train_loss:0.1452, test_acc:0.8319, test_loss:0.5727\n",
      "Epoch[106/200], train_acc:0.9539, train_loss:0.1353, test_acc:0.8258, test_loss:0.6193\n",
      "Epoch[107/200], train_acc:0.9541, train_loss:0.1361, test_acc:0.8228, test_loss:0.5995\n",
      "Epoch[108/200], train_acc:0.9554, train_loss:0.1331, test_acc:0.8252, test_loss:0.5878\n",
      "Epoch[109/200], train_acc:0.9566, train_loss:0.1327, test_acc:0.8277, test_loss:0.6177\n",
      "Epoch[110/200], train_acc:0.9590, train_loss:0.1236, test_acc:0.8358, test_loss:0.5685\n",
      "Epoch[111/200], train_acc:0.9600, train_loss:0.1206, test_acc:0.8077, test_loss:0.6579\n",
      "Epoch[112/200], train_acc:0.9565, train_loss:0.1282, test_acc:0.8434, test_loss:0.5396\n",
      "Epoch[113/200], train_acc:0.9629, train_loss:0.1125, test_acc:0.8268, test_loss:0.6264\n",
      "Epoch[114/200], train_acc:0.9626, train_loss:0.1117, test_acc:0.8103, test_loss:0.6965\n",
      "Epoch[115/200], train_acc:0.9582, train_loss:0.1234, test_acc:0.8237, test_loss:0.6035\n",
      "Epoch[116/200], train_acc:0.9651, train_loss:0.1035, test_acc:0.8112, test_loss:0.6474\n",
      "Epoch[117/200], train_acc:0.9631, train_loss:0.1107, test_acc:0.8146, test_loss:0.6417\n",
      "Epoch[118/200], train_acc:0.9653, train_loss:0.1038, test_acc:0.8186, test_loss:0.6476\n",
      "Epoch[119/200], train_acc:0.9669, train_loss:0.0981, test_acc:0.8279, test_loss:0.6054\n",
      "Epoch[120/200], train_acc:0.9653, train_loss:0.1034, test_acc:0.8375, test_loss:0.5954\n",
      "Epoch[121/200], train_acc:0.9691, train_loss:0.0923, test_acc:0.8325, test_loss:0.6096\n",
      "Epoch[122/200], train_acc:0.9713, train_loss:0.0863, test_acc:0.8141, test_loss:0.6806\n",
      "Epoch[123/200], train_acc:0.9671, train_loss:0.0963, test_acc:0.8330, test_loss:0.5784\n",
      "Epoch[124/200], train_acc:0.9735, train_loss:0.0804, test_acc:0.8371, test_loss:0.6182\n",
      "Epoch[125/200], train_acc:0.9719, train_loss:0.0855, test_acc:0.8161, test_loss:0.6423\n",
      "Epoch[126/200], train_acc:0.9725, train_loss:0.0821, test_acc:0.8208, test_loss:0.6321\n",
      "Epoch[127/200], train_acc:0.9700, train_loss:0.0887, test_acc:0.8382, test_loss:0.6023\n",
      "Epoch[128/200], train_acc:0.9750, train_loss:0.0772, test_acc:0.8385, test_loss:0.5861\n",
      "Epoch[129/200], train_acc:0.9772, train_loss:0.0711, test_acc:0.8378, test_loss:0.6062\n",
      "Epoch[130/200], train_acc:0.9774, train_loss:0.0693, test_acc:0.8294, test_loss:0.6203\n",
      "Epoch[131/200], train_acc:0.9775, train_loss:0.0679, test_acc:0.8371, test_loss:0.5987\n",
      "Epoch[132/200], train_acc:0.9795, train_loss:0.0637, test_acc:0.8426, test_loss:0.5766\n",
      "Epoch[133/200], train_acc:0.9788, train_loss:0.0663, test_acc:0.8244, test_loss:0.6390\n",
      "Epoch[134/200], train_acc:0.9824, train_loss:0.0540, test_acc:0.8122, test_loss:0.7625\n",
      "Epoch[135/200], train_acc:0.9742, train_loss:0.0771, test_acc:0.8281, test_loss:0.6236\n",
      "Epoch[136/200], train_acc:0.9838, train_loss:0.0513, test_acc:0.8214, test_loss:0.6556\n",
      "Epoch[137/200], train_acc:0.9811, train_loss:0.0572, test_acc:0.8208, test_loss:0.6560\n",
      "Epoch[138/200], train_acc:0.9782, train_loss:0.0655, test_acc:0.8324, test_loss:0.6042\n",
      "Epoch[139/200], train_acc:0.9885, train_loss:0.0388, test_acc:0.8446, test_loss:0.5785\n",
      "Epoch[140/200], train_acc:0.9893, train_loss:0.0351, test_acc:0.8146, test_loss:0.7209\n",
      "Epoch[141/200], train_acc:0.9871, train_loss:0.0413, test_acc:0.8349, test_loss:0.6346\n",
      "Epoch[142/200], train_acc:0.9898, train_loss:0.0351, test_acc:0.8364, test_loss:0.6198\n",
      "Epoch[143/200], train_acc:0.9847, train_loss:0.0488, test_acc:0.8321, test_loss:0.6343\n",
      "Epoch[144/200], train_acc:0.9860, train_loss:0.0444, test_acc:0.8411, test_loss:0.6046\n",
      "Epoch[145/200], train_acc:0.9875, train_loss:0.0400, test_acc:0.8293, test_loss:0.6511\n",
      "Epoch[146/200], train_acc:0.9859, train_loss:0.0443, test_acc:0.8330, test_loss:0.6463\n",
      "Epoch[147/200], train_acc:0.9889, train_loss:0.0362, test_acc:0.8421, test_loss:0.5968\n",
      "Epoch[148/200], train_acc:0.9959, train_loss:0.0159, test_acc:0.8561, test_loss:0.5426\n",
      "Epoch[149/200], train_acc:0.9975, train_loss:0.0106, test_acc:0.8656, test_loss:0.5089\n",
      "Epoch[150/200], train_acc:0.9997, train_loss:0.0033, test_acc:0.8810, test_loss:0.4376\n",
      "Epoch[151/200], train_acc:1.0000, train_loss:0.0016, test_acc:0.8842, test_loss:0.4150\n",
      "Epoch[152/200], train_acc:1.0000, train_loss:0.0017, test_acc:0.8857, test_loss:0.4031\n",
      "Epoch[153/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8871, test_loss:0.3981\n",
      "Epoch[154/200], train_acc:1.0000, train_loss:0.0018, test_acc:0.8890, test_loss:0.3849\n",
      "Epoch[155/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8901, test_loss:0.3839\n",
      "Epoch[156/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8909, test_loss:0.3777\n",
      "Epoch[157/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8900, test_loss:0.3801\n",
      "Epoch[158/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8906, test_loss:0.3749\n",
      "Epoch[159/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8906, test_loss:0.3753\n",
      "Epoch[160/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8912, test_loss:0.3740\n",
      "Epoch[161/200], train_acc:1.0000, train_loss:0.0021, test_acc:0.8925, test_loss:0.3782\n",
      "Epoch[162/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8928, test_loss:0.3725\n",
      "Epoch[163/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8929, test_loss:0.3761\n",
      "Epoch[164/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8925, test_loss:0.3726\n",
      "Epoch[165/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8931, test_loss:0.3721\n",
      "Epoch[166/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8935, test_loss:0.3742\n",
      "Epoch[167/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8929, test_loss:0.3713\n",
      "Epoch[168/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8917, test_loss:0.3719\n",
      "Epoch[169/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8924, test_loss:0.3705\n",
      "Epoch[170/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8924, test_loss:0.3701\n",
      "Epoch[171/200], train_acc:1.0000, train_loss:0.0020, test_acc:0.8910, test_loss:0.3780\n",
      "Epoch[172/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8915, test_loss:0.3753\n",
      "Epoch[173/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8930, test_loss:0.3702\n",
      "Epoch[174/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8921, test_loss:0.3701\n",
      "Epoch[175/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8911, test_loss:0.3721\n",
      "Epoch[176/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8923, test_loss:0.3766\n",
      "Epoch[177/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8913, test_loss:0.3743\n",
      "Epoch[178/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8914, test_loss:0.3720\n",
      "Epoch[179/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8915, test_loss:0.3744\n",
      "Epoch[180/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8905, test_loss:0.3737\n",
      "Epoch[181/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8934, test_loss:0.3721\n",
      "Epoch[182/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8937, test_loss:0.3709\n",
      "Epoch[183/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8920, test_loss:0.3730\n",
      "Epoch[184/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8926, test_loss:0.3706\n",
      "Epoch[185/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8927, test_loss:0.3717\n",
      "Epoch[186/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8930, test_loss:0.3707\n",
      "Epoch[187/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8919, test_loss:0.3752\n",
      "Epoch[188/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8919, test_loss:0.3759\n",
      "Epoch[189/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8930, test_loss:0.3737\n",
      "Epoch[190/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8936, test_loss:0.3691\n",
      "Epoch[191/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8935, test_loss:0.3716\n",
      "Epoch[192/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8938, test_loss:0.3685\n",
      "Epoch[193/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8931, test_loss:0.3743\n",
      "Epoch[194/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8930, test_loss:0.3700\n",
      "Epoch[195/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8947, test_loss:0.3681\n",
      "Epoch[196/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8930, test_loss:0.3712\n",
      "Epoch[197/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8936, test_loss:0.3693\n",
      "Epoch[198/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8927, test_loss:0.3733\n",
      "Epoch[199/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8914, test_loss:0.3726\n",
      "Epoch[200/200], train_acc:1.0000, train_loss:0.0019, test_acc:0.8925, test_loss:0.3733\n",
      "test_acc:0.8925, test_loss:0.3732\n",
      "[0.8925]\n"
     ]
    }
   ],
   "source": [
    "# standard ensemble CIFAR10 ResNet18\n",
    "M = 1\n",
    "standard_acc_list_c10_r18 = []\n",
    "standard_model_list_c10_r18 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = resnet18(10)\n",
    "    model_train(model, cifar10_resnet18, 200, 0.1, log=True)\n",
    "    standard_model_list_c10_r18.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar10_resnet18, acc=True)\n",
    "        standard_acc_list_c10_r18.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(standard_model_list_c10_r18, cifar10_resnet18, acc=True)\n",
    "        standard_acc_list_c10_r18.append(acc)\n",
    "\n",
    "print(standard_acc_list_c10_r18)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset_cifar10_resnet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni noise CIFAR10 ResNet18\n",
    "M = 10\n",
    "uni_acc_list_c10_r18 = []\n",
    "uni_model_list_c10_r18 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = resnet18(10)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_ResNet18_CIFAR10.pth\n",
    "    model = uni_noise(model, 'resnet18', 0.8, 1.6)\n",
    "    model_train(model, cifar10_resnet18, 50, 0.1, log=False)\n",
    "    uni_model_list_c10_r18.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar10_resnet18, acc=True)\n",
    "        uni_acc_list_c10_r18.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(uni_model_list_c10_r18, cifar10_resnet18, acc=True)\n",
    "        uni_acc_list_c10_r18.append(acc)\n",
    "        \n",
    "print(uni_acc_list_c10_r18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm noise CIFAR10 ResNet18\n",
    "M = 10\n",
    "norm_acc_list_c10_r18 = []\n",
    "norm_model_list_c10_r18 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = resnet18(10)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_ResNet18_CIFAR10.pth\n",
    "    model = norm_noise(model, 'resnet18', 0.8, 0.0, 1.6)\n",
    "    model_train(model, cifar10_resnet18, 50, 0.1, log=False)\n",
    "    norm_model_list_c10_r18.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar10_resnet18, acc=True)\n",
    "        norm_acc_list_c10_r18.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(norm_model_list_c10_r18, cifar10_resnet18, acc=True)\n",
    "        norm_acc_list_c10_r18.append(acc)\n",
    "        \n",
    "print(norm_acc_list_c10_r18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard ensemble CIFAR100 ResNet18\n",
    "M = 10\n",
    "standard_acc_list_c100_r18 = []\n",
    "standard_model_list_c100_r18 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = resnet18(100)\n",
    "    model_train(model, cifar100_resnet18, 200, 0.1, log=False)\n",
    "    standard_model_list_c100_r18.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar100_resnet18, acc=True)\n",
    "        standard_acc_list_c100_r18.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(standard_model_list_c100_r18, cifar100_resnet18, acc=True)\n",
    "        standard_acc_list_c100_r18.append(acc)\n",
    "\n",
    "print(standard_acc_list_c100_r18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni noise CIFAR100 ResNet18\n",
    "M = 10\n",
    "uni_acc_list_c100_r18 = []\n",
    "uni_model_list_c100_r18 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = resnet18(100)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_ResNet18_CIFAR100.pth\n",
    "    model = uni_noise(model, 'resnet18', 0.8, 1.6)\n",
    "    model_train(model, cifar100_resnet18, 50, 0.1, log=False)\n",
    "    uni_model_list_c100_r18.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar100_resnet18, acc=True)\n",
    "        uni_acc_list_c100_r18.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(uni_model_list_c10_r18, cifar100_resnet18, acc=True)\n",
    "        uni_acc_list_c100_r18.append(acc)\n",
    "        \n",
    "print(uni_acc_list_c100_r18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm noise CIFAR100 ResNet18\n",
    "M = 10\n",
    "norm_acc_list_c100_r18 = []\n",
    "norm_model_list_c100_r18 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = resnet18(100)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_ResNet18_CIFAR100.pth\n",
    "    model = norm_noise(model, 'resnet18', 0.8, 0.0, 1.6)\n",
    "    model_train(model, cifar100_resnet18, 50, 0.1, log=False)\n",
    "    norm_model_list_c100_r18.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar100_resnet18, acc=True)\n",
    "        norm_acc_list_c100_r18.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(norm_model_list_c100_r18, cifar100_resnet18, acc=True)\n",
    "        norm_acc_list_c100_r18.append(acc)\n",
    "        \n",
    "print(norm_acc_list_c100_r18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard ensemble CIFAR10 VGG16 \n",
    "M = 10\n",
    "standard_acc_list_c10_v16 = []\n",
    "standard_model_list_c10_v16 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = vgg16(10)\n",
    "    model_train(model, cifar10_vgg16, 200, 0.01, log=False)\n",
    "    standard_model_list_c10_v16.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar10_vgg16, acc=True)\n",
    "        standard_acc_list_c10_v16.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(standard_model_list_c10_v16, cifar10_vgg16, acc=True)\n",
    "        standard_acc_list_c10_v16.append(acc)\n",
    "\n",
    "print(standard_acc_list_c10_v16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni noise CIFAR10 VGG16\n",
    "M = 10\n",
    "uni_acc_list_c10_v16 = []\n",
    "uni_model_list_c10_v16 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = vgg16(10)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_VGG16_CIFAR10.pth\n",
    "    model = uni_noise(model, 'vgg16', 0.1, 0.1)\n",
    "    model_train(model, cifar10_vgg16, 50, 0.01, log=False)\n",
    "    uni_model_list_c10_v16.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar10_vgg16, acc=True)\n",
    "        uni_acc_list_c10_v16.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(uni_model_list_c10_v16, cifar10_vgg16, acc=True)\n",
    "        uni_acc_list_c10_v16.append(acc)\n",
    "\n",
    "print(uni_acc_list_c10_v16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm noise CIFAR10 VGG16\n",
    "M = 10\n",
    "norm_acc_list_c10_v16 = []\n",
    "norm_model_list_c10_v16 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = vgg16(10)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_VGG16_CIFAR10.pth\n",
    "    model = norm_noise(model, 'vgg16', 0.1, 0.0, 0.01)\n",
    "    model_train(model, cifar10_vgg16, 50, 0.01, log=False)\n",
    "    norm_model_list_c10_v16.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar10_vgg16, acc=True)\n",
    "        norm_acc_list_c10_v16.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(norm_model_list_c10_v16, cifar10_vgg16, acc=True)\n",
    "        norm_acc_list_c10_v16.append(acc)\n",
    "\n",
    "print(norm_acc_list_c10_v16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard ensemble CIFAR100 VGG16 \n",
    "M = 10\n",
    "standard_acc_list_c100_v16 = []\n",
    "standard_model_list_c100_v16 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = vgg16(100)\n",
    "    model_train(model, cifar100_vgg16, 200, 0.01, log=False)\n",
    "    standard_model_list_c100_v16.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar100_vgg16, acc=True)\n",
    "        standard_acc_list_c100_v16.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(standard_model_list_c100_v16, cifar100_vgg16, acc=True)\n",
    "        standard_acc_list_c100_v16.append(acc)\n",
    "\n",
    "print(standard_acc_list_c100_v16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni noise CIFAR100 VGG16\n",
    "M = 10\n",
    "uni_acc_list_c100_v16 = []\n",
    "uni_model_list_c100_v16 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = vgg16(100)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_VGG16_CIFAR100.pth\n",
    "    model = uni_noise(model, 'vgg16', 0.1, 0.1)\n",
    "    model_train(model, cifar100_vgg16, 50, 0.01, log=False)\n",
    "    uni_model_list_c100_v16.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar100_vgg16, acc=True)\n",
    "        uni_acc_list_c100_v16.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(uni_model_list_c100_v16, cifar100_vgg16, acc=True)\n",
    "        uni_acc_list_c100_v16.append(acc)\n",
    "\n",
    "print(uni_acc_list_c100_v16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm noise CIFAR100 VGG16\n",
    "M = 10\n",
    "norm_acc_list_c100_v16 = []\n",
    "norm_model_list_c100_v16 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = vgg16(100)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_VGG16_CIFAR100.pth\n",
    "    model = norm_noise(model, 'vgg16', 0.05, 0.0, 0.05)\n",
    "    model_train(model, cifar100_vgg16, 50, 0.01, log=False)\n",
    "    norm_model_list_c100_v16.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar100_vgg16, acc=True)\n",
    "        norm_acc_list_c100_v16.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(norm_model_list_c100_v16, cifar100_vgg16, acc=True)\n",
    "        norm_acc_list_c100_v16.append(acc)\n",
    "\n",
    "print(norm_acc_list_c100_v16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard ensemble CIFAR10 EfficientNetB0\n",
    "M = 10\n",
    "standard_acc_list_c10_eb0 = []\n",
    "standard_model_list_c10_eb0 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = efficientnetb0(10)\n",
    "    model_train(model, cifar10_efficient, 100, 0.1, log=False)\n",
    "    standard_model_list_c10_eb0.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar10_efficient, acc=True)\n",
    "        standard_acc_list_c10_eb0.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(standard_model_list_c10_eb0, cifar10_efficient, acc=True)\n",
    "        standard_acc_list_c10_eb0.append(acc)\n",
    "\n",
    "print(standard_acc_list_c10_eb0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni noise CIFAR10 EfficientNetB0\n",
    "M = 10\n",
    "\n",
    "uni_acc_list_c10_eb0 = []\n",
    "uni_model_list_c10_eb0 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = efficientnetb0(10)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_EfficientNetB0_CIFAR100.pth\n",
    "    model = uni_noise(model, 'efficientnetb0', 0.7, 0.1)\n",
    "    model_train(model, cifar10_efficient, 25, 0.1, log=False)\n",
    "    uni_model_list_c10_eb0.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar10_efficient, acc=True)\n",
    "        uni_acc_list_c10_eb0.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(uni_model_list_c10_eb0, cifar10_efficient, acc=True)\n",
    "        uni_acc_list_c10_eb0.append(acc)\n",
    "\n",
    "print(uni_acc_list_c10_eb0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm noise CIFAR10 EfficientNetB0\n",
    "M = 10\n",
    "\n",
    "norm_acc_list_c10_eb0 = []\n",
    "norm_model_list_c10_eb0 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = efficientnetb0(10)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_EfficientNetB0_CIFAR100.pth\n",
    "    model = norm_noise(model, 'efficientnetb0', 0.3, 0.0, 0.1)\n",
    "    model_train(model, cifar10_efficient, 25, 0.1, log=False)\n",
    "    norm_model_list_c10_eb0.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar10_efficient, acc=True)\n",
    "        norm_acc_list_c10_eb0.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(norm_model_list_c10_eb0, cifar10_efficient, acc=True)\n",
    "        norm_acc_list_c10_eb0.append(acc)\n",
    "\n",
    "print(norm_acc_list_c10_eb0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard ensemble CIFAR100 EfficientNetB0\n",
    "M = 10\n",
    "standard_acc_list_c100_eb0 = []\n",
    "standard_model_list_c100_eb0 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = efficientnetb0(100)\n",
    "    model_train(model, cifar100_efficient, 100, 0.1, log=False)\n",
    "    standard_model_list_c100_eb0.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar100_efficient, acc=True)\n",
    "        standard_acc_list_c100_eb0.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(standard_model_list_c100_eb0, cifar100_efficient, acc=True)\n",
    "        standard_acc_list_c100_eb0.append(acc)\n",
    "\n",
    "print(standard_acc_list_c100_eb0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uni noise CIFAR100 EfficientNetB0\n",
    "M = 10\n",
    "\n",
    "uni_acc_list_c100_eb0 = []\n",
    "uni_model_list_c100_eb0 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = efficientnetb0(100)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_EfficientNetB0_CIFAR100.pth\n",
    "    model = uni_noise(model, 'efficientnetb0', 0.3, 0.3)\n",
    "    model_train(model, cifar100_efficient, 25, 0.1, log=False)\n",
    "    uni_model_list_c100_eb0.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar100_efficient, acc=True)\n",
    "        uni_acc_list_c100_eb0.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(uni_model_list_c100_eb0, cifar100_efficient, acc=True)\n",
    "        uni_acc_list_c100_eb0.append(acc)\n",
    "\n",
    "print(uni_acc_list_c100_eb0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norm noise CIFAR100 EfficientNetB0\n",
    "M = 10\n",
    "\n",
    "norm_acc_list_c100_eb0 = []\n",
    "norm_model_list_c100_eb0 = []\n",
    "\n",
    "for i in range(M):\n",
    "    model = efficientnetb0(100)\n",
    "    model.load_state_dict(torch.load()) # model_pretrain_EfficientNetB0_CIFAR100.pth\n",
    "    model = norm_noise(model, 'efficientnetb0', 0.3, 0.0, 0.3)\n",
    "    model_train(model, cifar100_efficient, 25, 0.1, log=False)\n",
    "    norm_model_list_c100_eb0.append(model)\n",
    "    if i == 0:\n",
    "        acc = model_test(model, cifar100_efficient, acc=True)\n",
    "        norm_acc_list_c100_eb0.append(acc)\n",
    "    else:\n",
    "        acc = ens_test(norm_model_list_c100_eb0, cifar100_efficient, acc=True)\n",
    "        norm_acc_list_c100_eb0.append(acc)\n",
    "\n",
    "print(norm_acc_list_c100_eb0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
